{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "express-charlotte",
   "metadata": {},
   "source": [
    "## Importing all packages and setting environment for GPU usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614be752",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "from transformers import DistilBertTokenizer, DistilBertForTokenClassification\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support, accuracy_score, classification_report\n",
    "import torch\n",
    "import argparse\n",
    "from os import path\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import random\n",
    "\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:3\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    \n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "international-township",
   "metadata": {},
   "source": [
    "## Helper functions and model selection for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "still-department",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fluent(s,label):\n",
    "    new_s = []\n",
    "    words = s.split(\" \")\n",
    "    for x,y in zip(words,label):\n",
    "        if(y==0):\n",
    "            new_s.append(x)\n",
    "        \n",
    "    return \" \".join(new_s)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smart-passenger",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"dc\"\n",
    "model_checkpoint = \"google/muril-base-cased\"\n",
    "model_checkpoint = \"xlm-roberta-base\"\n",
    "#model_checkpoint = \"bert-base-multilingual-cased\"\n",
    "\n",
    "path_to_dataset = \"./data/\"\n",
    "\n",
    "print(\"Model Name:\", model_checkpoint)\n",
    "experiment_id = \"123456\"\n",
    "no_of_epochs = 40\n",
    "\n",
    "batch_size = 16\n",
    "label_list = ['is_fluent', 'is_disfluent'] # 0 -> isFluent , 1 -> isDisfluent\n",
    "splits = [80,10,10]\n",
    "\n",
    "test_sentences = []\n",
    "gt_fluent_sentences = []\n",
    "\n",
    "def get_dataset(path):\n",
    "    \n",
    "    with open(f\"{path}/data.dis\", 'r') as dis_x, open(f\"{path}/data.labels\", 'r') as labels_x:\n",
    "\n",
    "        train_dict = {'labels': [], 'disfluent': []}\n",
    "        valid_dict = {'labels': [], 'disfluent': []}\n",
    "        test_dict  = {'labels': [], 'disfluent': []}\n",
    "        \n",
    "        disfluent_lines = dis_x.readlines()\n",
    "        disfluent_labels = labels_x.readlines()\n",
    "        temp = list(zip(disfluent_lines, disfluent_labels))\n",
    "        random.shuffle(temp)\n",
    "        res1, res2 = zip(*temp)\n",
    "        # res1 and res2 come out as tuples, and so must be converted to lists.\n",
    "        disfluent_lines, disfluent_labels = list(res1), list(res2)\n",
    "        total_size = len(disfluent_lines)\n",
    "        \n",
    "        for i,(disfluent,labels) in enumerate(zip(disfluent_lines, disfluent_labels)):\n",
    "\n",
    "            disfluent = disfluent.strip().split()\n",
    "            labels = list(map(int, labels.strip().split()))\n",
    "\n",
    "            if i < round(splits[0] * total_size / 100) :\n",
    "                train_dict['disfluent'].append(disfluent)\n",
    "                train_dict['labels'].append(labels)\n",
    "            elif i < round(sum(splits[:2]) * total_size / 100):\n",
    "                valid_dict['disfluent'].append(disfluent)\n",
    "                valid_dict['labels'].append(labels)\n",
    "            elif i < round(sum(splits) * total_size / 100):\n",
    "                test_dict['disfluent'].append(disfluent)\n",
    "                test_dict['labels'].append(labels)   \n",
    "                s = \" \".join(disfluent)\n",
    "                test_sentences.append(\" \".join(disfluent))\n",
    "                gt_fluent_sentences.append(get_fluent(s,labels))\n",
    "                \n",
    "                \n",
    "    train_dataset = Dataset.from_dict(train_dict)\n",
    "    valid_dataset = Dataset.from_dict(valid_dict)\n",
    "    test_dataset = Dataset.from_dict(test_dict)\n",
    "\n",
    "    return DatasetDict({'train': train_dataset, 'valid': valid_dataset, 'test': test_dataset})\n",
    "\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"disfluent\"], truncation=True, is_split_into_words=True)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"labels\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n",
    "            # ignored in the loss function.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            # We set the label for the first token of each word.\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            # For the other tokens in a word, we set the label to either the current label or -100, depending on\n",
    "            # the label_all_tokens flag.\n",
    "            else:\n",
    "                label_ids.append(label[word_idx] if label_all_tokens else -100)\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "datasets = get_dataset(path=\"/home/development/vineet/DDP_1/presto_v1/test_partitions/de-DE/\")\n",
    "print(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adolescent-mongolia",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Data Size = \"+str(len(datasets['train'])+len(datasets['valid'])+len(datasets['test'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accepting-tattoo",
   "metadata": {},
   "outputs": [],
   "source": [
    "\" \".join(datasets['test'][0]['disfluent'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "close-desert",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "patient-spotlight",
   "metadata": {},
   "source": [
    "## Setting up training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6282e477",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "label_all_tokens = True\n",
    "tokenized_datasets = datasets.map(tokenize_and_align_labels, batched=True)\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(label_list))\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "\n",
    "args = TrainingArguments(\n",
    "    f\"checkpoints/{experiment_id}\",\n",
    "    evaluation_strategy = \"steps\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=no_of_epochs,#10,\n",
    "    weight_decay=0.01,\n",
    "    eval_steps=1000, #1000,\n",
    "    logging_steps=1000, #1000,\n",
    "    save_steps=1000, #1000,\n",
    "    save_total_limit=1,\n",
    "    load_best_model_at_end=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3d882e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "remarkable-maintenance",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sized-check",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets['train']['disfluent'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1dbcd19",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Args\", args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92a800e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # 1-d prediction & true label\n",
    "    true_predictions = [\n",
    "        p for prediction, label in zip(predictions, labels) for (p, l) in zip(prediction, label) if l != -100\n",
    "    ]\n",
    "    true_labels = [\n",
    "        l for prediction, label in zip(predictions, labels) for (p, l) in zip(prediction, label) if l != -100 \n",
    "    ]\n",
    "\n",
    "    results = precision_recall_fscore_support(true_labels, true_predictions, zero_division=0)\n",
    "    return {\n",
    "        'accuracy': accuracy_score(true_labels, true_predictions),\n",
    "        'precision0': torch.tensor(results[0])[0],\n",
    "        'precision1': torch.tensor(results[0])[1],\n",
    "        'recall0': torch.tensor(results[1])[0],\n",
    "        'recall1': torch.tensor(results[1])[1],\n",
    "        'f1score0': torch.tensor(results[2])[0],\n",
    "        'f1score1': torch.tensor(results[2])[1],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "floral-essay",
   "metadata": {},
   "source": [
    "## Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "younger-least",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"valid\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "double-muscle",
   "metadata": {},
   "source": [
    "## Run Evaluation on Test Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "round-madness",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test sentences\n",
    "predictions, labels, _ = trainer.predict(tokenized_datasets[\"test\"])\n",
    "predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "# 1-d prediction & true label\n",
    "true_predictions = [\n",
    "    p for prediction, label in zip(predictions, labels) for (p, l) in zip(prediction, label) if l != -100\n",
    "]\n",
    "true_labels = [\n",
    "    l for prediction, label in zip(predictions, labels) for (p, l) in zip(prediction, label) if l != -100 \n",
    "]\n",
    "\n",
    "results = precision_recall_fscore_support(true_labels, true_predictions, zero_division=0)\n",
    "print({\n",
    "    'precision': results[0],\n",
    "    'recall': results[1],\n",
    "    'f1score': results[2]\n",
    "})\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(true_labels, true_predictions, normalize='all'))\n",
    "print(classification_report(true_labels, true_predictions, target_names=label_list, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "occupied-baseline",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(true_labels, true_predictions, normalize='all')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
